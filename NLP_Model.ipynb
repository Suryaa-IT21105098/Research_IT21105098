{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REzdSDQczy0m"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import classification_report\n",
        "import torch\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_excel('NLP.xlsx')  # Replace 'NLP.xlsx' with the path to your Excel dataset\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df.drop(df.columns[df.columns.str.startswith('Unnamed:')], axis=1, inplace=True)\n",
        "\n",
        "# Drop rows with missing values in description column\n",
        "df.dropna(subset=['description'], inplace=True)\n",
        "\n",
        "# Convert non-string values to strings in the 'description' column\n",
        "df['description'] = df['description'].astype(str)\n",
        "\n",
        "# Define input text and labels\n",
        "X = df['description']\n",
        "y = df[['inclusion_criteria', 'incident_type', 'receiver_name', 'receiver_country', 'receiver_category', 'initiator_name', 'initiator_category', 'attributing_actor', 'attributed_initiator', 'zero_days', 'MITRE_initial_access', 'MITRE_impact', 'user_interaction', 'has_disruption', 'data_theft', 'disruption', 'hijacking', 'target_multiplier', 'impact_indicator']]\n",
        "\n",
        "# Print original length of y\n",
        "print(\"Original length of y:\", len(y))\n",
        "\n",
        "# Convert labels to the appropriate data type\n",
        "for column in y.columns:\n",
        "    y[column] = pd.to_numeric(y[column], errors='coerce')\n",
        "\n",
        "# Drop rows with missing values in labels\n",
        "y.dropna(inplace=True)\n",
        "\n",
        "# Print final length of y after dropping missing values\n",
        "print(\"Final length of y after preprocessing:\", len(y))\n",
        "\n",
        "# Reset indices to ensure alignment\n",
        "X.reset_index(drop=True, inplace=True)\n",
        "y.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Print lengths of X and y after reset indices\n",
        "print(\"Length of X:\", len(X))\n",
        "print(\"Length of y:\", len(y))\n",
        "\n",
        "# Ensure the number of samples in X matches the number of samples in y\n",
        "if len(X) != len(y):\n",
        "    raise ValueError(\"Number of samples in input text and labels do not match!\")\n",
        "\n",
        "# Convert labels to multi-hot encoding\n",
        "mlb = MultiLabelBinarizer()\n",
        "labels = mlb.fit_transform(y.values)\n",
        "\n",
        "# Load pre-trained BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=labels.shape[1])\n",
        "\n",
        "# Tokenize input text\n",
        "inputs = tokenizer(X.tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# Convert labels to tensors\n",
        "labels = torch.tensor(labels, dtype=torch.float32)\n",
        "\n",
        "# Split data into train and test sets\n",
        "train_inputs, test_inputs, train_labels, test_labels = train_test_split(inputs, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    logging_dir='./logs',\n",
        ")\n",
        "\n",
        "# Define Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_inputs,\n",
        "    eval_dataset=test_inputs,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "predictions = trainer.predict(test_inputs)\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(test_labels, predictions.predictions > 0.5, target_names=mlb.classes_))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import classification_report\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_excel('NLP.xlsx')  # Replace 'NLP.xlsx' with the path to your Excel dataset\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df.drop(df.columns[df.columns.str.startswith('Unnamed:')], axis=1, inplace=True)\n",
        "\n",
        "# Drop rows with missing values in description column\n",
        "df.dropna(subset=['description'], inplace=True)\n",
        "\n",
        "# Convert non-string values to strings in the 'description' column\n",
        "df['description'] = df['description'].astype(str)\n",
        "\n",
        "# Define input text and labels\n",
        "X = df['description']\n",
        "y = df[['inclusion_criteria', 'incident_type', 'receiver_name', 'receiver_country']]\n",
        "\n",
        "# Convert non-numeric values to strings\n",
        "for column in y.columns:\n",
        "    y[column] = y[column].astype(str)\n",
        "\n",
        "# Convert labels to multi-hot encoding\n",
        "mlb = MultiLabelBinarizer()\n",
        "labels = mlb.fit_transform(y.values)\n",
        "\n",
        "# Load pre-trained BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize input text\n",
        "inputs = tokenizer(X.tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# Ensure inputs and labels have the same number of samples\n",
        "assert inputs.input_ids.shape[0] == labels.shape[0], \"Number of samples in inputs and labels do not match!\"\n",
        "\n",
        "# Split data into train and test sets\n",
        "train_indices, test_indices = train_test_split(range(len(X)), test_size=0.2, random_state=42)\n",
        "train_inputs = {key: value[train_indices] for key, value in inputs.items()}\n",
        "train_labels = labels[train_indices]\n",
        "test_inputs = {key: value[test_indices] for key, value in inputs.items()}\n",
        "test_labels = labels[test_indices]\n",
        "\n",
        "# Create PyTorch DataLoader\n",
        "train_dataset = TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], torch.tensor(train_labels, dtype=torch.float))\n",
        "test_dataset = TensorDataset(test_inputs['input_ids'], test_inputs['attention_mask'], torch.tensor(test_labels, dtype=torch.float))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8)\n",
        "\n",
        "# Load pre-trained BERT model\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(mlb.classes_))\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Training loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(3):  # Adjust number of epochs as needed\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for batch in train_loader:\n",
        "        input_ids, attention_mask, targets = batch\n",
        "        input_ids, attention_mask, targets = input_ids.to(device), attention_mask.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=targets)\n",
        "        loss = criterion(outputs.logits, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "predictions = []\n",
        "true_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids, attention_mask, targets = batch\n",
        "        input_ids, attention_mask, targets = input_ids.to(device), attention_mask.to(device), targets.to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predictions.extend(torch.sigmoid(outputs.logits).cpu().numpy())\n",
        "        true_labels.extend(targets.cpu().numpy())\n",
        "\n",
        "predictions = torch.tensor(predictions) > 0.5\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(true_labels, predictions, target_names=mlb.classes_))\n"
      ],
      "metadata": {
        "id": "SAKvE_0Rz-SW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_excel('NLP.xlsx')  # Replace 'NLP.xlsx' with the path to your Excel dataset\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df.drop(df.columns[df.columns.str.startswith('Unnamed:')], axis=1, inplace=True)\n",
        "\n",
        "# Drop rows with missing values in description column\n",
        "df.dropna(subset=['description'], inplace=True)\n",
        "\n",
        "# Convert non-string values to strings in the 'description' column\n",
        "df['description'] = df['description'].astype(str)\n",
        "\n",
        "# Define input text and labels\n",
        "X = df['description']\n",
        "y = df[['inclusion_criteria', 'incident_type', 'receiver_name', 'receiver_country']]\n",
        "\n",
        "# Convert non-numeric values to strings\n",
        "for column in y.columns:\n",
        "    y[column] = y[column].astype(str)\n",
        "\n",
        "# Convert labels to multi-hot encoding\n",
        "mlb = MultiLabelBinarizer()\n",
        "labels = mlb.fit_transform(y.values)\n",
        "\n",
        "# Load pre-trained BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize input text\n",
        "inputs = tokenizer(X.tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# Ensure inputs and labels have the same number of samples\n",
        "assert inputs.input_ids.shape[0] == labels.shape[0], \"Number of samples in inputs and labels do not match!\"\n",
        "\n",
        "# Sample a subset of the dataset\n",
        "X_sampled, _, y_sampled, _ = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert non-numeric values in labels to numeric\n",
        "label_encoder = LabelEncoder()\n",
        "labels_numeric = label_encoder.fit_transform(y_sampled.values)\n",
        "\n",
        "# Split data into train and test sets\n",
        "train_indices, test_indices = train_test_split(range(len(X)), test_size=0.2, random_state=42)\n",
        "\n",
        "# Split inputs and labels based on indices\n",
        "train_inputs = {key: value[train_indices] for key, value in inputs.items()}\n",
        "train_labels = labels[train_indices]\n",
        "test_inputs = {key: value[test_indices] for key, value in inputs.items()}\n",
        "test_labels = labels[test_indices]\n",
        "\n",
        "# Load pre-trained BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(y.columns))\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Tokenize input text\n",
        "inputs = tokenizer(X_sampled.tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
        "labels = y_sampled.values\n",
        "\n",
        "# Convert labels to numpy array\n",
        "labels_array = y_sampled.values\n",
        "\n",
        "# Create PyTorch DataLoader\n",
        "dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'], torch.tensor(labels, dtype=torch.float))\n",
        "sampler = RandomSampler(dataset)\n",
        "batch_size = 4  # Adjust batch_size as needed\n",
        "dataloader = DataLoader(dataset, sampler=sampler, batch_size=batch_size)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Training loop\n",
        "model.train()\n",
        "for epoch in range(3):  # Adjust number of epochs as needed\n",
        "    for step, batch in enumerate(dataloader):\n",
        "        input_ids, attention_mask, targets = batch\n",
        "        input_ids, attention_mask, targets = input_ids.to(device), attention_mask.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=targets)\n",
        "        loss = criterion(outputs.logits, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Epoch {epoch+1}, Step {step}, Loss: {loss.item()}\")\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "predictions = []\n",
        "true_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids, attention_mask, targets = batch\n",
        "        input_ids, attention_mask, targets = input_ids.to(device), attention_mask.to(device), targets.to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predictions.extend(torch.sigmoid(outputs.logits).cpu().numpy())\n",
        "        true_labels.extend(targets.cpu().numpy())\n",
        "\n",
        "predictions = torch.tensor(predictions) > 0.5\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(true_labels, predictions, target_names=mlb.classes_))\n"
      ],
      "metadata": {
        "id": "fSxHZy9Pz--W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}